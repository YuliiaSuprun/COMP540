{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdcb5f6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-24T16:42:18.598140Z",
     "iopub.status.busy": "2023-10-24T16:42:18.597758Z",
     "iopub.status.idle": "2023-10-24T16:42:23.020670Z",
     "shell.execute_reply": "2023-10-24T16:42:23.019806Z"
    },
    "papermill": {
     "duration": 4.429287,
     "end_time": "2023-10-24T16:42:23.023063",
     "exception": false,
     "start_time": "2023-10-24T16:42:18.593776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d9b157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T16:42:23.029492Z",
     "iopub.status.busy": "2023-10-24T16:42:23.029046Z",
     "iopub.status.idle": "2023-10-24T16:42:23.033429Z",
     "shell.execute_reply": "2023-10-24T16:42:23.032703Z"
    },
    "papermill": {
     "duration": 0.009598,
     "end_time": "2023-10-24T16:42:23.035301",
     "exception": false,
     "start_time": "2023-10-24T16:42:23.025703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n",
    "# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n",
    "\n",
    "if DEVICE != 'cuda':\n",
    "    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbbbab7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T16:42:23.040970Z",
     "iopub.status.busy": "2023-10-24T16:42:23.040670Z",
     "iopub.status.idle": "2023-10-24T16:42:23.052077Z",
     "shell.execute_reply": "2023-10-24T16:42:23.051397Z"
    },
    "papermill": {
     "duration": 0.016293,
     "end_time": "2023-10-24T16:42:23.053924",
     "exception": false,
     "start_time": "2023-10-24T16:42:23.037631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for loading the hidden dataset.\n",
    "\n",
    "def load_example(df_row):\n",
    "    image = torchvision.io.read_image(df_row['image_path'])\n",
    "    result = {\n",
    "        'image': image,\n",
    "        'image_id': df_row['image_id'],\n",
    "        'age_group': df_row['age_group'],\n",
    "        'age': df_row['age'],\n",
    "        'person_id': df_row['person_id']\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "class HiddenDataset(Dataset):\n",
    "    '''The hidden dataset.'''\n",
    "    def __init__(self, split='train'):\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "\n",
    "        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n",
    "        df['image_path'] = df['image_id'].apply(\n",
    "            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n",
    "        df = df.sort_values(by='image_path')\n",
    "        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n",
    "        if len(self.examples) == 0:\n",
    "            raise ValueError('No examples.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        image = example['image']\n",
    "        image = image.to(torch.float32)\n",
    "        example['image'] = image\n",
    "        return example\n",
    "\n",
    "\n",
    "def get_dataset(batch_size):\n",
    "    '''Get the dataset.'''\n",
    "    retain_ds = HiddenDataset(split='retain')\n",
    "    forget_ds = HiddenDataset(split='forget')\n",
    "    val_ds = HiddenDataset(split='validation')\n",
    "\n",
    "    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n",
    "    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return retain_loader, forget_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6c8236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T16:42:23.059298Z",
     "iopub.status.busy": "2023-10-24T16:42:23.059044Z",
     "iopub.status.idle": "2023-10-24T16:42:23.067959Z",
     "shell.execute_reply": "2023-10-24T16:42:23.067069Z"
    },
    "papermill": {
     "duration": 0.014107,
     "end_time": "2023-10-24T16:42:23.070211",
     "exception": false,
     "start_time": "2023-10-24T16:42:23.056104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unlearning(\n",
    "    net, \n",
    "    retain_loader, \n",
    "    forget_loader, \n",
    "    val_loader):\n",
    "    \"\"\"Enhanced unlearning method.\"\"\"\n",
    "    \n",
    "    epochs = 3\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    net.train()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        # Finetune on retain set\n",
    "        for sample in retain_loader:\n",
    "            inputs = sample[\"image\"]\n",
    "            targets = sample[\"age_group\"]\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Negative fine-tuning on the forget set\n",
    "        for sample in forget_loader:\n",
    "            inputs = sample[\"image\"]\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            # Randomly assign wrong labels\n",
    "            targets = torch.randint(0, 10, (inputs.size(0),)).to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc26699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T16:42:23.077559Z",
     "iopub.status.busy": "2023-10-24T16:42:23.077187Z",
     "iopub.status.idle": "2023-10-24T16:42:23.096332Z",
     "shell.execute_reply": "2023-10-24T16:42:23.095417Z"
    },
    "papermill": {
     "duration": 0.025771,
     "end_time": "2023-10-24T16:42:23.098828",
     "exception": false,
     "start_time": "2023-10-24T16:42:23.073057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n",
    "    # mock submission\n",
    "    subprocess.run('touch submission.zip', shell=True)\n",
    "else:\n",
    "    \n",
    "    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n",
    "    # as otherwise this notebook may fail due to running out of disk space.\n",
    "    # The below code saves them in /kaggle/tmp to avoid that issue.\n",
    "    \n",
    "    os.makedirs('/kaggle/tmp', exist_ok=True)\n",
    "    retain_loader, forget_loader, validation_loader = get_dataset(64)\n",
    "    net = resnet18(weights=None, num_classes=10)\n",
    "    net.to(DEVICE)\n",
    "    for i in range(512):\n",
    "        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n",
    "        unlearning(net, retain_loader, forget_loader, validation_loader)\n",
    "        state = net.state_dict()\n",
    "        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n",
    "        \n",
    "    # Ensure that submission.zip will contain exactly 512 checkpoints \n",
    "    # (if this is not the case, an exception will be thrown).\n",
    "    unlearned_ckpts = os.listdir('/kaggle/tmp')\n",
    "    if len(unlearned_ckpts) != 512:\n",
    "        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n",
    "        \n",
    "    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.117242,
   "end_time": "2023-10-24T16:42:24.321779",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-24T16:42:15.204537",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
