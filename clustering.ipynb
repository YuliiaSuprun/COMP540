{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb47d9e",
   "metadata": {
    "_cell_guid": "c0ec4268-4934-4706-a1bd-dd5ff1ed49a5",
    "_uuid": "b7975139-72f6-4502-9ec1-1c050d54f7da",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-25T02:31:58.071249Z",
     "iopub.status.busy": "2023-10-25T02:31:58.070884Z",
     "iopub.status.idle": "2023-10-25T02:32:03.932689Z",
     "shell.execute_reply": "2023-10-25T02:32:03.931725Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.869879,
     "end_time": "2023-10-25T02:32:03.935089",
     "exception": false,
     "start_time": "2023-10-25T02:31:58.065210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78dbc68",
   "metadata": {
    "_cell_guid": "6a214a4f-e60e-4d32-ab35-e6e22d4b080f",
    "_uuid": "bb9061ee-0b27-4113-b7c4-1a4a5b3317d2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-25T02:32:03.944191Z",
     "iopub.status.busy": "2023-10-25T02:32:03.943684Z",
     "iopub.status.idle": "2023-10-25T02:32:03.948631Z",
     "shell.execute_reply": "2023-10-25T02:32:03.947800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011429,
     "end_time": "2023-10-25T02:32:03.950554",
     "exception": false,
     "start_time": "2023-10-25T02:32:03.939125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n",
    "# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n",
    "\n",
    "if DEVICE != 'cuda':\n",
    "    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d95d320",
   "metadata": {
    "_cell_guid": "79bd834d-ceeb-4fcf-8195-f2c7e2c802bb",
    "_uuid": "c4872aca-f576-4a5f-bcaa-d2996ee941b0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-25T02:32:03.958981Z",
     "iopub.status.busy": "2023-10-25T02:32:03.958655Z",
     "iopub.status.idle": "2023-10-25T02:32:03.976921Z",
     "shell.execute_reply": "2023-10-25T02:32:03.975992Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025312,
     "end_time": "2023-10-25T02:32:03.979359",
     "exception": false,
     "start_time": "2023-10-25T02:32:03.954047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for loading the hidden dataset.\n",
    "\n",
    "def load_example(df_row):\n",
    "    image = torchvision.io.read_image(df_row['image_path'])\n",
    "    result = {\n",
    "        'image': image,\n",
    "        'image_id': df_row['image_id'],\n",
    "        'age_group': df_row['age_group'],\n",
    "        'age': df_row['age'],\n",
    "        'person_id': df_row['person_id']\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "class HiddenDataset(Dataset):\n",
    "    '''The hidden dataset.'''\n",
    "    def __init__(self, split='train'):\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "\n",
    "        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n",
    "        df['image_path'] = df['image_id'].apply(\n",
    "            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n",
    "        df = df.sort_values(by='image_path')\n",
    "        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n",
    "        if len(self.examples) == 0:\n",
    "            raise ValueError('No examples.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        image = example['image']\n",
    "        image = image.to(torch.float32)\n",
    "        example['image'] = image\n",
    "        return example\n",
    "\n",
    "\n",
    "def get_dataset(batch_size):\n",
    "    '''Get the dataset.'''\n",
    "    retain_ds = HiddenDataset(split='retain')\n",
    "    forget_ds = HiddenDataset(split='forget')\n",
    "    val_ds = HiddenDataset(split='validation')\n",
    "\n",
    "    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n",
    "    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return retain_loader, forget_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d3aadf",
   "metadata": {
    "_cell_guid": "09b9bae1-d02e-4e34-b7df-34e61d2072f8",
    "_uuid": "730c0bc2-65f1-4a82-a269-946baefd3318",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-25T02:32:03.989668Z",
     "iopub.status.busy": "2023-10-25T02:32:03.989377Z",
     "iopub.status.idle": "2023-10-25T02:32:03.996947Z",
     "shell.execute_reply": "2023-10-25T02:32:03.996047Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014275,
     "end_time": "2023-10-25T02:32:03.998959",
     "exception": false,
     "start_time": "2023-10-25T02:32:03.984684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(model, loader):\n",
    "    \"\"\"Extracts features from the penultimate layer of the model for the given loader.\"\"\"\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Remove the final classification layer to get features\n",
    "    feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in loader:\n",
    "            images = sample['image'].to(DEVICE)\n",
    "            lbls = sample['age_group']\n",
    "            features = feature_extractor(images)\n",
    "            features = features.view(features.size(0), -1).cpu().numpy()\n",
    "            features_list.extend(features)\n",
    "            labels_list.extend(lbls.cpu().numpy())\n",
    "\n",
    "    return features_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "331e6d0a",
   "metadata": {
    "_cell_guid": "ecfa71a0-d53e-4a62-9bfc-5f6214f1b280",
    "_uuid": "7762353a-4146-4d33-8d62-9a9e13c83d66",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-25T02:32:04.009038Z",
     "iopub.status.busy": "2023-10-25T02:32:04.008698Z",
     "iopub.status.idle": "2023-10-25T02:32:04.016127Z",
     "shell.execute_reply": "2023-10-25T02:32:04.015380Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013954,
     "end_time": "2023-10-25T02:32:04.018100",
     "exception": false,
     "start_time": "2023-10-25T02:32:04.004146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_data(model, loader, n_clusters=100):\n",
    "    \"\"\"Groups the data into 'n_clusters' clusters using KMeans clustering on the extracted features.\"\"\"\n",
    "    # Extract features from the model\n",
    "    features, labels = extract_features(model, loader)\n",
    "\n",
    "    # Use KMeans to cluster the extracted features\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10).fit(features)\n",
    "    clusters = kmeans.predict(features)\n",
    "\n",
    "    # Convert clusters and labels to tensors\n",
    "    clusters = torch.tensor(clusters, dtype=torch.long)\n",
    "\n",
    "    # Get the data tensor\n",
    "    data_list = [sample['image'] for sample in loader.dataset]\n",
    "    data_tensor = torch.stack(data_list).view(-1, 3, 32, 32)\n",
    "\n",
    "    return data_tensor, labels, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a63627c4",
   "metadata": {
    "_cell_guid": "f917e64e-6959-49a4-ad19-4d5331bdd61b",
    "_uuid": "da496ee3-3f69-40f5-a4be-1399d4adf777",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-25T02:32:04.027992Z",
     "iopub.status.busy": "2023-10-25T02:32:04.027452Z",
     "iopub.status.idle": "2023-10-25T02:32:04.039759Z",
     "shell.execute_reply": "2023-10-25T02:32:04.038850Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018998,
     "end_time": "2023-10-25T02:32:04.041602",
     "exception": false,
     "start_time": "2023-10-25T02:32:04.022604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unlearning1(net, retain_loader, forget_loader, validation_loader, n_clusters=100):\n",
    "    # Vectorized.\n",
    "    data, labels, clusters = group_data(net, retain_loader, n_clusters)\n",
    "    data, clusters = data.to(DEVICE), clusters.to(DEVICE)\n",
    "\n",
    "    epochs = 1\n",
    "    epochs2 = 3\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "    # scheduler = StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    net.train()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        for sample in forget_loader:\n",
    "            inputs = sample['image'].to(DEVICE)\n",
    "            targets = sample['age_group'].to(DEVICE)\n",
    "            \n",
    "            # First Optimization Phase\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            random_targets = (targets + torch.randint(1, 10, targets.size(), device=DEVICE)) % 10\n",
    "            loss = criterion(outputs, random_targets)\n",
    "            loss.backward()\n",
    "            # Gradient Clipping\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Map targets to cluster labels\n",
    "            cluster_labels = torch.tensor([clusters[labels.index(t.item())] for t in targets], device=DEVICE)\n",
    "            \n",
    "            # Get a mask for each cluster label\n",
    "            masks = [(clusters == label).nonzero(as_tuple=True)[0] for label in cluster_labels.unique()]\n",
    "\n",
    "            for ep2 in range(epochs2):\n",
    "                for mask in masks:\n",
    "                    same_cluster_data = torch.index_select(data, 0, mask).to(DEVICE)\n",
    "                    same_cluster_labels = torch.tensor(labels, device=DEVICE)[mask]\n",
    "\n",
    "                    # Second Optimization Phase\n",
    "                    optimizer.zero_grad()\n",
    "                    output = net(same_cluster_data)\n",
    "                    loss = criterion(output, same_cluster_labels)\n",
    "                    loss.backward()\n",
    "                    # Gradient Clipping\n",
    "                    nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                \n",
    "    # Start validation phase\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999a84f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T02:32:04.050648Z",
     "iopub.status.busy": "2023-10-25T02:32:04.050015Z",
     "iopub.status.idle": "2023-10-25T02:32:04.064058Z",
     "shell.execute_reply": "2023-10-25T02:32:04.063260Z"
    },
    "papermill": {
     "duration": 0.020445,
     "end_time": "2023-10-25T02:32:04.065927",
     "exception": false,
     "start_time": "2023-10-25T02:32:04.045482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unlearning(net, retain_loader, forget_loader, validation_loader, n_clusters=100):\n",
    "    # Vectorized.\n",
    "    data, labels, clusters = group_data(net, retain_loader, n_clusters)\n",
    "    data, clusters = data.to(DEVICE), clusters.to(DEVICE)    \n",
    "\n",
    "    epochs = 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "    # scheduler = StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    net.train()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Negative fine-tuning\n",
    "        for sample in forget_loader:\n",
    "            inputs = sample['image'].to(DEVICE)\n",
    "            targets = sample['age_group'].to(DEVICE)\n",
    "            \n",
    "            # First Optimization Phase\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            random_targets = (targets + torch.randint(1, 10, targets.size(), device=DEVICE)) % 10\n",
    "            loss = criterion(outputs, random_targets)\n",
    "            loss.backward()\n",
    "            # Gradient Clipping\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Map targets to cluster labels\n",
    "            cluster_labels = torch.tensor([clusters[labels.index(t.item())] for t in targets], device=DEVICE)\n",
    "            \n",
    "            # Get a mask for each cluster label\n",
    "            masks = [(clusters == label).nonzero(as_tuple=True)[0] for label in cluster_labels.unique()]\n",
    "\n",
    "            for mask in masks:\n",
    "                same_cluster_data = torch.index_select(data, 0, mask).to(DEVICE)\n",
    "                same_cluster_labels = torch.tensor(labels, device=DEVICE)[mask]\n",
    "\n",
    "                # Second Optimization Phase\n",
    "                optimizer.zero_grad()\n",
    "                output = net(same_cluster_data)\n",
    "                loss = criterion(output, same_cluster_labels)\n",
    "                loss.backward()\n",
    "                # Gradient Clipping\n",
    "                nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "                optimizer.step()\n",
    "                \n",
    "        # Positive fine-tuning    \n",
    "        for sample in retain_loader:\n",
    "            inputs = sample[\"image\"].to(DEVICE)\n",
    "            targets = sample[\"age_group\"].to(DEVICE)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "                \n",
    "    # Start validation phase\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63a6de1",
   "metadata": {
    "_cell_guid": "a98adec5-615a-463b-b12c-f69f72e4c0b8",
    "_uuid": "da943cc6-932d-43ea-a18c-bd8254f7860c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-25T02:32:04.074201Z",
     "iopub.status.busy": "2023-10-25T02:32:04.073936Z",
     "iopub.status.idle": "2023-10-25T02:32:04.088941Z",
     "shell.execute_reply": "2023-10-25T02:32:04.088059Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021403,
     "end_time": "2023-10-25T02:32:04.090933",
     "exception": false,
     "start_time": "2023-10-25T02:32:04.069530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n",
    "    # mock submission\n",
    "    subprocess.run('touch submission.zip', shell=True)\n",
    "else:\n",
    "    \n",
    "    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n",
    "    # as otherwise this notebook may fail due to running out of disk space.\n",
    "    # The below code saves them in /kaggle/tmp to avoid that issue.\n",
    "    \n",
    "    os.makedirs('/kaggle/tmp', exist_ok=True)\n",
    "    retain_loader, forget_loader, validation_loader = get_dataset(64)\n",
    "    net = resnet18(weights=None, num_classes=10)\n",
    "    net.to(DEVICE)\n",
    "    \n",
    "    initial_clusters = 10\n",
    "    max_clusters = 500\n",
    "    step = (max_clusters - initial_clusters) // 511\n",
    "\n",
    "    for i in range(512):\n",
    "        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n",
    "        n_clusters = initial_clusters + i * step\n",
    "        unlearning(net, retain_loader, forget_loader, validation_loader, n_clusters)\n",
    "        state = net.state_dict()\n",
    "        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n",
    "        \n",
    "    # Ensure that submission.zip will contain exactly 512 checkpoints \n",
    "    # (if this is not the case, an exception will be thrown).\n",
    "    unlearned_ckpts = os.listdir('/kaggle/tmp')\n",
    "    if len(unlearned_ckpts) != 512:\n",
    "        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n",
    "        \n",
    "    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.799835,
   "end_time": "2023-10-25T02:32:05.516501",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-25T02:31:54.716666",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
