{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htU_mI7-gCbj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.cluster import KMeans\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "GB449m7pgLpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(model, loader):\n",
        "    \"\"\"Extracts features from the penultimate layer of the model for the given loader.\"\"\"\n",
        "    model.eval()\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Remove the final classification layer to get features\n",
        "    feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, lbls in loader:\n",
        "            images = images.to(DEVICE)\n",
        "            features = feature_extractor(images)\n",
        "            features = features.view(features.size(0), -1).cpu().numpy()\n",
        "            features_list.extend(features)\n",
        "            labels_list.extend(lbls.cpu().numpy())\n",
        "\n",
        "    return features_list, labels_list"
      ],
      "metadata": {
        "id": "6p7ypLiQ02Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_data_old(loader, n_clusters):\n",
        "    \"\"\"Groups the data into 'n_clusters' clusters using KMeans clustering.\"\"\"\n",
        "    data_list, labels = [], []\n",
        "\n",
        "    for sample in loader:\n",
        "        images, lbls = sample\n",
        "        data_list.extend(images.view(images.size(0), -1).cpu().numpy())\n",
        "        labels.extend(lbls.cpu().numpy())\n",
        "\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=10).fit(data_list)\n",
        "    clusters = kmeans.predict(data_list)\n",
        "\n",
        "    data_tensor = torch.tensor(data_list).view(-1, 3, 32, 32)\n",
        "    return data_tensor, labels, clusters\n"
      ],
      "metadata": {
        "id": "LcJkVkiYgO36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_data(model, loader, n_clusters=100):\n",
        "    \"\"\"Groups the data into 'n_clusters' clusters using KMeans clustering on the extracted features.\"\"\"\n",
        "    # Extract features from the model\n",
        "    features, labels = extract_features(model, loader)\n",
        "\n",
        "    # Use KMeans to cluster the extracted features\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=10).fit(features)\n",
        "    clusters = kmeans.predict(features)\n",
        "\n",
        "    # Convert clusters and labels to tensors\n",
        "    clusters = torch.tensor(clusters, dtype=torch.long)\n",
        "\n",
        "    # Get the data tensor\n",
        "    data_list = [sample[0] for sample in loader.dataset]\n",
        "    data_tensor = torch.stack(data_list).view(-1, 3, 32, 32)\n",
        "\n",
        "    return data_tensor, labels, clusters"
      ],
      "metadata": {
        "id": "nkOmjj-o0_6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unlearning(net, retain_loader, forget_loader, n_clusters=100):\n",
        "    # Not vectorized!!!!\n",
        "    data, labels, clusters = group_data(net, retain_loader, n_clusters)\n",
        "    print(\"data.shape: \", data.shape)\n",
        "    epochs = 1\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
        "    net.train()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        for inputs, targets in forget_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            random_targets = (targets + torch.randint(1, 10, targets.size(), device=DEVICE)) % 10\n",
        "            loss = criterion(outputs, random_targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            for idx, target in enumerate(targets):\n",
        "              cluster_label = clusters[labels.index(target.item())]\n",
        "              same_cluster_indices = [i for i, cluster in enumerate(clusters) if cluster == cluster_label]\n",
        "              print(\"same_cluster_indices: \", same_cluster_indices)\n",
        "              print(\"len(same_cluster_indices)\", len(same_cluster_indices))\n",
        "              same_cluster_data = data[same_cluster_indices].to(DEVICE)\n",
        "              print(\"same_cluster_data.shape: \", same_cluster_data.shape)\n",
        "              same_cluster_labels = torch.tensor([labels[i] for i in same_cluster_indices], device=DEVICE)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              output = net(same_cluster_data)\n",
        "              loss = criterion(output, same_cluster_labels)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "    net.eval()"
      ],
      "metadata": {
        "id": "y-lEPi2DgSOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unlearning(net, retain_loader, forget_loader, validation_loader, n_clusters=100):\n",
        "  # Vectorized.\n",
        "    data, labels, clusters = group_data(net, retain_loader, n_clusters)\n",
        "    data, clusters = data.to(DEVICE), clusters.to(DEVICE)\n",
        "\n",
        "    epochs = 1\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
        "    # scheduler = StepLR(optimizer, step_size=10, gamma=0.7)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    net.train()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        for inputs, targets in forget_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "            # First Optimization Phase\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            random_targets = (targets + torch.randint(1, 10, targets.size(), device=DEVICE)) % 10\n",
        "            loss = criterion(outputs, random_targets)\n",
        "            loss.backward()\n",
        "            # Gradient Clipping\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Map targets to cluster labels\n",
        "            cluster_labels = torch.tensor([clusters[labels.index(t.item())] for t in targets], device=DEVICE)\n",
        "\n",
        "            # Get a mask for each cluster label\n",
        "            masks = [(clusters == label).nonzero(as_tuple=True)[0] for label in cluster_labels.unique()]\n",
        "\n",
        "\n",
        "            for mask in masks:\n",
        "                same_cluster_data = torch.index_select(data, 0, mask).to(DEVICE)\n",
        "                same_cluster_labels = torch.tensor(labels, device=DEVICE)[mask]\n",
        "\n",
        "                # Second Optimization Phase\n",
        "                optimizer.zero_grad()\n",
        "                output = net(same_cluster_data)\n",
        "                loss = criterion(output, same_cluster_labels)\n",
        "                loss.backward()\n",
        "                # Gradient Clipping\n",
        "                nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "        # Start validation phase\n",
        "        net.eval()  # set the model to evaluation mode\n",
        "        total_val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # no need to compute gradients during validation\n",
        "            for inputs, targets in validation_loader:\n",
        "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = net(inputs)\n",
        "                val_loss = criterion(outputs, targets)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "                # Optionally, compute accuracy\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(validation_loader)\n",
        "        val_accuracy = 100. * correct / total\n",
        "\n",
        "        print(f\"Epoch {ep + 1}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "        net.train()  # set the model back to training mode"
      ],
      "metadata": {
        "id": "KF7lwOHY43k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading CIFAR-10 dataset\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "cifar10_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Splitting the CIFAR-10 training set into retain and forget subsets\n",
        "retain_dataset, forget_dataset = random_split(cifar10_dataset, [40000, 10000])\n",
        "\n",
        "retain_loader = DataLoader(retain_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "forget_loader = DataLoader(forget_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnvQou9igbiH",
        "outputId": "4af0d5c8-b5eb-4be9-bbfb-ebe29a530481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 16032963.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_iter = iter(retain_loader)\n",
        "images, labels = next(data_iter)\n",
        "print(images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXGDVur3pR8i",
        "outputId": "6c7c96c3-96f1-4b97-abb1-3ca83a13b91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = resnet18(pretrained=True) # Load pre-trained on ImageNet\n",
        "# Adjust for CIFAR-10\n",
        "num_ftrs = net.fc.in_features\n",
        "net.fc = nn.Linear(num_ftrs, 10)\n",
        "net.to(DEVICE)"
      ],
      "metadata": {
        "id": "Mi2RwKUIgc-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlearning(net, retain_loader, forget_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYM08j2Di3-o",
        "outputId": "dbec4622-8df8-4dbb-b56e-b31d507a1510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 4.8504, Validation Accuracy: 10.97%\n"
          ]
        }
      ]
    }
  ]
}